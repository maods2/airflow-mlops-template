# Guia de ApresentaÃ§Ã£o: Astronomer Airflow
## Orquestrando Data Engineering e Machine Learning na Nuvem

**DuraÃ§Ã£o:** 40 minutos  
**Cargo:** Data Engineer and Scientist

---

## Estrutura da ApresentaÃ§Ã£o

### 1. **IntroduÃ§Ã£o: O que Ã© Astronomer Airflow?** (5 min)
- Contexto do Apache Airflow
- Como Astronomer surgiu e suas vantagens
- Por que usar Astronomer para MLOps?

**Demo:** Mostrar o Airflow UI e estrutura do projeto

### 2. **Setup e ConfiguraÃ§Ã£o** (8 min)
- Infraestrutura (Docker, Astronomer Runtime)
- Estrutura do projeto
- DAGs, Executors, configuraÃ§Ã£o
- Observabilidade (logs, mÃ©tricas, UI)

**Demo:** Navegar pelo projeto, mostrar Dockerfile, estrutura de pastas

### 3. **Tutorial BÃ¡sico de Airflow** (10 min)
- CriaÃ§Ã£o de DAGs
- Operadores (Python, BigQuery, Dataproc)
- DependÃªncias entre tarefas
- Dynamic Task Mapping
- VariÃ¡veis e Macros do Airflow

**Demo:** Criar um DAG simples, mostrar tasks, executar e ver logs

### 4. **Data Engineering + Machine Learning** (10 min)
- AutomaÃ§Ã£o de ETL
- Feature Engineering
- Training Pipeline
- Versionamento de Modelos
- Deploy e InferÃªncia

**Demo:** Mostrar DAGs completos: ingestÃ£o, treino, inferÃªncia

### 5. **IntegraÃ§Ã£o com GCP** (5 min)
- BigQuery: Armazenamento e consultas
- Dataproc: Processamento paralelo (Spark)
- Vertex AI: Training e Model Registry

**Demo:** Executar pipeline que usa todas essas tecnologias

### 6. **Q&A** (2 min)

---

## Script de Demo

### Setup Inicial (Antes de ComeÃ§ar)

```bash
# Navegar atÃ© o diretÃ³rio do projeto
cd d:\dev\data-science\airflow-mlops-template

# Build da imagem Docker
docker build -f astro/Dockerfile -t airflow-demo .
```

---

## Segmento 1: IntroduÃ§Ã£o (5 min)

### Slides/Fala:
"O Apache Airflow Ã© uma plataforma open-source para orquestraÃ§Ã£o de workflows. O Astronomer, fundado pelos criadores do Airflow, oferece uma versÃ£o enterprise com recursos adicionais e melhor suporte."

### Demo:
```bash
# Mostrar estrutura do projeto
ls -la
# Mostrar:
# - astro/ (configuraÃ§Ã£o do Airflow)
# - feature-code/ (cÃ³digo ML e SQL)
# - README.md (documentaÃ§Ã£o)
```

### Mostrar na UI do Airflow:
- VisÃ£o geral dos DAGs
- Links rÃ¡pidos para a documentaÃ§Ã£o
- Astronomer Runtime version

---

## Segmento 2: Setup e ConfiguraÃ§Ã£o (8 min)

### Slides/Fala:
"Vamos entender como estÃ¡ estruturado nosso ambiente Astronomer Airflow."

### Demo:
1. **Mostrar Dockerfile**
```dockerfile
# Destacar:
# - Base image Astronomer Runtime
# - DependÃªncias (requirements.txt)
# - Estrutura de pastas
```

2. **Mostrar Estrutura de Pastas**
```
astro/
â”œâ”€â”€ dags/              # DAGs do Airflow
â”œâ”€â”€ config/            # ConfiguraÃ§Ãµes YAML
â”œâ”€â”€ dag_builder/       # Gerador de DAGs dinÃ¢micos
â”œâ”€â”€ templates/         # Templates Jinja2
â””â”€â”€ Dockerfile         # Build da imagem

feature-code/
â”œâ”€â”€ src/               # CÃ³digo Python (train, predict, preprocess)
â””â”€â”€ sql/               # Queries SQL
```

3. **Mostrar ConfiguraÃ§Ã£o YAML (ingestion.yaml)**
```yaml
# Destacar:
# - DAG metadata (nome, schedule, tags)
# - Environment-specific configs
# - Task definitions com SQL files
```

4. **Mostrar Airflow UI**
- Acessar http://localhost:8080
- Mostrar Admin -> Variables
- Mostrar Admin -> Connections
- Mostrar DAGs na interface

---

## Segmento 3: Tutorial BÃ¡sico (10 min)

### Demo 1: DAG Simples com Python Operator

**Slides/Fala:** "Vamos criar nosso primeiro DAG simples usando o TaskFlow API."

**Arquivo:** `astro/dags/demo_simple.py`

```python
from airflow.decorators import dag, task
from pendulum import datetime

@dag(
    dag_id="demo_simple",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
)
def demo_simple():
    @task
    def extract():
        return {"data": "hello from task 1"}

    @task
    def transform(data):
        return {"transformed": data["data"].upper()}

    @task
    def load(data):
        print(f"Loading: {data}")

    data = extract()
    transformed = transform(data)
    load(transformed)

demo_simple()
```

**AÃ§Ã£o na UI:**
1. Mostrar o DAG na interface
2. Trigger o DAG manualmente
3. Mostrar execuÃ§Ã£o das tasks
4. Mostrar logs de cada task
5. Mostrar XCom para ver dados compartilhados

---

### Demo 2: BigQuery Operator

**Slides/Fala:** "Agora vamos adicionar integraÃ§Ã£o com BigQuery."

**Mostrar:** arquivo `astro/dags/exampledag.py` (jÃ¡ existe)

**AÃ§Ã£o na UI:**
1. Mostrar o DAG `example_astronauts`
2. Mostrar cÃ³digo (API call, XCom, dynamic mapping)
3. Trigger e executar
4. Mostrar BigQuery Console para ver resultados

---

### Demo 3: DependÃªncias Complexas

**Slides/Fala:** "Vamos ver dependÃªncias entre tarefas."

**Mostrar:** `astro/dags/data_ingestion_pipeline.py`

**Pontos a destacar:**
- Extracting data
- Validating data
- Loading to warehouse
- Dependencies: extract â†’ validate â†’ load
- Mostrar grÃ¡fico de dependÃªncias no UI

---

## Segmento 4: Data Engineering + ML (10 min)

### Demo 1: Pipeline de IngestÃ£o de Dados

**Arquivo:** `astro/dags/data_ingestion_pipeline.py`

**Executar:**
1. Mostrar o DAG
2. Mostrar configuraÃ§Ã£o YAML (`astro/config/ingestion.yaml`)
3. Mostrar SQL files (`feature-code/sql/ingestion_*.sql`)
4. Trigger o DAG
5. Destacar:
   - BigQuery queries
   - Data quality validation
   - Incremental loads (WRITE_APPEND)
   - Partitioned tables

---

### Demo 2: Pipeline de ML Training

**Arquivo:** Config `astro/config/training.yaml`

**Slides/Fala:** "Agora vamos combinar Data Engineering com ML."

**Mostrar arquivos:**
1. `training.yaml` - configuraÃ§Ã£o completa
2. `feature-code/src/train/train.py` - cÃ³digo de treino
3. `feature-code/src/preprocess/preprocess.py` - feature engineering

**Pontos a destacar:**
- BigQuery: preparar dados de treino
- Dataproc: processar com Spark
- Feature engineering
- Model training
- Model evaluation e mÃ©tricas
- Model versioning

---

### Demo 3: Pipeline de InferÃªncia

**Arquivo:** `astro/config/model_inference.yaml`

**Mostrar:**
1. Batch inference com modelo treinado
2. Post-processing de predictions
3. Salvando predictions no BigQuery

---

## Segmento 5: IntegraÃ§Ã£o GCP (5 min)

### Demo: Pipeline Completo E2E

**Slides/Fala:** "Vamos executar um pipeline end-to-end que usa todas as ferramentas GCP."

**Passos:**
1. **BigQuery** - ExtraÃ§Ã£o e transformaÃ§Ã£o de dados
2. **Dataproc** - Feature engineering distribuÃ­do
3. **Vertex AI** - Training do modelo
4. **BigQuery** - Salvando mÃ©tricas
5. **Dataproc** - Batch inference
6. **BigQuery** - Predictions finais

**Mostrar na UI:**
- DAG completo com todas as integraÃ§Ãµes
- Logs de cada operador
- Status de execuÃ§Ã£o
- GrÃ¡fico de dependÃªncias
- Tempo de execuÃ§Ã£o

**Mostrar no Console GCP:**
- BigQuery: tabelas criadas
- Dataproc: jobs executados
- Vertex AI: modelos registrados

---

## Segmento 6: Q&A (2 min)

**Pontos para destacar se perguntarem:**

1. **Escalabilidade:** Como escalar com Kubernetes, mÃºltiplos workers
2. **Observabilidade:** IntegraÃ§Ã£o com Datadog, Prometheus, Grafana
3. **CI/CD:** Deployment automatizado, testes de DAGs
4. **Custos:** OtimizaÃ§Ã£o de Dataproc clusters, cache do BigQuery
5. **Versionamento:** Git para cÃ³digo, MLflow para modelos

---

## Comandos Ãšteis Durante a Demo

### Iniciar Airflow Local
```bash
# Se estiver usando Astro CLI
astro dev start

# Ou com Docker
docker run -p 8080:8080 airflow-demo
```

### Ver Logs
```bash
# Terminal
docker logs <container_id>

# UI do Airflow: clicar em task â†’ logs
```

### Executar DAGs
```bash
# CLI do Airflow
airflow dags trigger <dag_id>

# Ou pela UI: clicar em "Trigger DAG"
```

### Verificar ConfiguraÃ§Ãµes
```bash
# Environment variables
docker exec <container> env | grep GCP

# Airflow connections
airflow connections list

# Airflow variables
airflow variables list
```

---

## Checklist Antes da ApresentaÃ§Ã£o

- [ ] Docker build funcionando
- [ ] Airflow UI acessÃ­vel na porta 8080
- [ ] Credenciais GCP configuradas
- [ ] DAGs aparecendo na interface
- [ ] Exemplo de DAGs rodando
- [ ] BigQuery projeto configurado
- [ ] Dataproc permissions configuradas
- [ ] Slides preparados
- [ ] Backup de screenshots da UI
- [ ] CÃ³digo de exemplo funcionando

---

## Recursos Adicionais

### Links Ãšteis
- [Astronomer Docs](https://docs.astronomer.io/)
- [Airflow Docs](https://airflow.apache.org/)
- [GCP Integration](https://cloud.google.com/composer/docs)
- [Astronomer Academy](https://www.astronomer.io/learn/)

### CÃ³digo de ReferÃªncia
- `astro/dags/` - DAGs de exemplo
- `astro/config/` - ConfiguraÃ§Ãµes
- `feature-code/src/` - CÃ³digo ML
- `feature-code/sql/` - SQL queries

---

## Estrutura Recomendada para ApresentaÃ§Ã£o

```
1. IntroduÃ§Ã£o e Contexto           (5 min)
   â”œâ”€ Astronomer vs Apache Airflow
   â”œâ”€ VisÃ£o geral da arquitetura
   â””â”€ Demo: UI do Airflow

2. Setup e Infraestrutura          (8 min)
   â”œâ”€ Docker e Astronomer Runtime
   â”œâ”€ Estrutura do projeto
   â”œâ”€ ConfiguraÃ§Ãµes YAML
   â””â”€ Demo: Navegar pelo cÃ³digo

3. Tutorial BÃ¡sico                  (10 min)
   â”œâ”€ DAGs simples
   â”œâ”€ Operadores (Python, BigQuery)
   â”œâ”€ DependÃªncias
   â””â”€ Demo: Criar e executar DAG

4. Data Engineering + ML            (10 min)
   â”œâ”€ Pipeline ETL
   â”œâ”€ Training pipeline
   â”œâ”€ InferÃªncia
   â””â”€ Demo: Pipeline completo

5. IntegraÃ§Ã£o GCP                   (5 min)
   â”œâ”€ BigQuery
   â”œâ”€ Dataproc
   â”œâ”€ Vertex AI
   â””â”€ Demo: E2E pipeline

6. Q&A                              (2 min)
   â””â”€ TÃ³picos adicionais
```

---

**Boa apresentaÃ§Ã£o! ðŸš€**

