"""
{{ description }}

Generated Dataproc DAG: {{ dag_name }}
"""

from airflow.sdk import dag, task
from airflow.providers.google.cloud.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocSubmitJobOperator,
    DataprocDeleteClusterOperator
)
from pendulum import datetime
from typing import Any, Dict

# Default arguments
default_args = {
    "owner": "{{ default_args.owner | default('airflow') }}",
    "retries": {{ default_args.retries | default(1) }},
    "retry_delay": "{{ default_args.retry_delay | default('00:05:00') }}",
    "email": {{ default_args.email | default([]) }},
    "email_on_failure": {{ default_args.email_on_failure | default(false) }},
    "email_on_retry": {{ default_args.email_on_retry | default(false) }}
}

@dag(
    dag_id="{{ dag_name }}",
    start_date=datetime({{ start_date | default('2024, 1, 1') }}),
    schedule="{{ schedule | default('@daily') }}",
    default_args=default_args,
    description="{{ description | default('Dataproc DAG') }}",
    tags={{ tags | default([]) }},
    catchup=False,
    max_active_runs=1
)
def {{ dag_name | snake_case }}():
    # Create cluster
    create_cluster = DataprocCreateClusterOperator(
        task_id="create_cluster",
        project_id="{{ project_id }}",
        cluster_config={
            "master_config": {
                "num_instances": 1,
                "machine_type_uri": "n1-standard-2",
                "disk_config": {
                    "boot_disk_type": "pd-standard",
                    "boot_disk_size_gb": 100
                }
            },
            "worker_config": {
                "num_instances": 2,
                "machine_type_uri": "n1-standard-2",
                "disk_config": {
                    "boot_disk_type": "pd-standard",
                    "boot_disk_size_gb": 100
                }
            }
        },
        region="{{ region }}",
        cluster_name="{{ cluster_name }}",
        gcp_conn_id="google_cloud_default"
    )
    
    {% for job in jobs %}
    {{ job.task_id | snake_case }} = DataprocSubmitJobOperator(
        task_id="{{ job.task_id }}",
        project_id="{{ project_id }}",
        job={
            "reference": {"job_id": "{{ job.task_id }}-{{ ts_nodash }}"},
            "placement": {"cluster_name": "{{ cluster_name }}"},
            "{{ job.job_type }}_job": {
                {% if job.job_type == 'pyspark' %}
                "main_python_file_uri": "{{ job.main_python_file_uri }}",
                {% elif job.job_type == 'spark' %}
                "main_class": "{{ job.main_class }}",
                "jar_file_uris": {{ job.jar_file_uris | default([]) }},
                {% endif %}
                "args": {{ job.args | default([]) }}
            }
        },
        region="{{ region }}",
        gcp_conn_id="google_cloud_default"
    )
    {% endfor %}
    
    # Delete cluster
    delete_cluster = DataprocDeleteClusterOperator(
        task_id="delete_cluster",
        project_id="{{ project_id }}",
        cluster_name="{{ cluster_name }}",
        region="{{ region }}",
        gcp_conn_id="google_cloud_default",
        trigger_rule="all_done"
    )
    
    # Define task dependencies
    create_cluster >> [
        {% for job in jobs %}
        {{ job.task_id | snake_case }}{% if not loop.last %},{% endif %}
        {% endfor %}
    ] >> delete_cluster
    
    {% for job in jobs %}
    {% if job.depends_on %}
    {% for dep in job.depends_on %}
    {{ dep | snake_case }} >> {{ job.task_id | snake_case }}
    {% endfor %}
    {% endif %}
    {% endfor %}

# Instantiate the DAG
{{ dag_name | snake_case }}()
